---
title: "Groupwork_2_Group6"
author: "Luca Wirths, Lars Wenger, Josua Reich, Abishan Arumugavel "
date: "2023-05-21"
output:
  html_document:
    toc: yes
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of content {.tabset}

## Preparations


### Import Modules
```{r, include=FALSE}
#install.packages("readr")
#install.packages("dplyr")
#install.packages("dlookr")
#install.packages("naniar")
#install.packages("UpSetR")
#install.packages("ggplot2")
#install.packages("tidyr")
#install.packages("mice")
#install.packages("purrr")
#install.packages("scales")

library(readr)
library(dplyr)
library(dlookr)
library(naniar)
library(ggplot2)
library(tidyr)
library(purrr)
library(Boruta)
library(DescTools)
library(data.table)
library(h2o)

```


### Import Data
```{r}
load("data_wage.RData")
data_wage <- data #copy of dataset
set.seed(8)
```

## Analyse Data

```{r}
overview(data)
summary(data)
```

In the data are 10809 observations of 78 variables. 

Most of the respondents in the dataset are male (9135). Rest of it are female (1571), prefered "not to say" (72) or did "self-describe" (31). Thus the men are strongly overrepresented. However, this is probably due to the industry, where a larger proportion of men are employed. A similar overrepresentation can be seen in the country of origin. There is also a clear majority there with USA (2505) and India (1576).
The dataset is somewhat more balanced with regard to the age of the participants. Most participants are between 25-29 years old (3008), but the remaining age groups are also well represented.
It is also clear that about half of the participants have a master's degree (5209). However, there is also a good proportion of participants with a bachelor's degree (2990) or a doctorate (1869).
Most participants have studied "Computer Science" (4239), "Engineering" (1704) or "Mathematics or Statistics" (1545). 
In terms of job title, there are different designations with "Data Scientist" (2505), "Software Engineer" (1800), "Student" (1588) and "Data Analyst" (1022).
The majority of the participants still have little professional experience. Thus, more than half of the participants have less than 3 years of experience.


### Check data

We now investigated the data on plausability. 
To have a better feeling and see more in detail how the feature is distributed we show here the plot_outlier, which gives us a boxplot and a histogram with and without outliers.

```{r,echo=FALSE}
#Let's visualize the wage columnns with and without outliers
data %>%
  plot_outlier(col="aquamarine3",diagnose_outlier(data['wage']) %>%
                 filter(outliers_ratio >= 0.5) %>%          # dplyr
                 select(variables) %>%
                 unlist())

```

We see that without the outliers the standard distribution is better.
We see the problem in the surveys that both the job description and the industry can indicate that you are a student. How to handle this was probably not immediately clear to all respondents.
That's why we check how plausible the information in the survey can be. On the one hand, we look at how many people who are not students have stated that they do not earn anything. We also look at how many students stated that they earn more than 150,000. Both are combinations that we do not consider plausible. We see that there are such entries and therefore assume that the outliers are partly due to incorrect entries.


```{r, echo=FALSE}
# Check how many non-students have wage = 0 
num_rows_1 <- nrow(data[data$wage == 0 & !(data$job_role == "Student" | data$industry == "I am a student"), ])
print(paste("Number of **Non** student's, with wage 0:", num_rows_1))

# Check how many students have wage > 150'000
num_rows_2 <- nrow(data[data$wage > 150000 & (data$job_role == "Student" | data$industry == "I am a student"), ])
print(paste("Number of student's, with wage more than 150,000:", num_rows_2))
```

We see those two possibilities to now ether delete those entries or we use an outlierhandling and give them a wage.

### 10% outliers

Since we have already looked at the outlier-distribution above, we think it is better to use the outlierhandling.
So we create now the standard function to replace the outliers with the 10th and 90th percentile value of that wage feature. The default values for the outlierhandling would be 5/95% but, we have a little below 10% of data with "0$" as wage, so we had to change it to 10/90% values.

```{r,echo=FALSE}
outlier <- function(x){
  quantiles <- quantile(x, c(.1, .9))
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  x
}   
```

In the next step, we apply the outlier function. For this, we use the map_df function. This allows us to apply a function to each element of a list or atomic vector.

```{r, echo=FALSE}

#Use function outlier for the dataset
data['wage'] <- map_df(data['wage'], outlier)

#Save to later use on 
data_outliers10 <- data
```

Let's look at the distribution of the wage now:

```{r, echo=FALSE}
# Let's check our dependent variable "wage"
ggplot(data_outliers10, aes(x = wage)) + 
  geom_histogram(fill="aquamarine3", color = "black") +
  labs(title = "Histogram of the wages for option 1") +
  theme(legend.position="none")
```


With this outlierhandling, the students now make around 3200 $ / year. Which we think is a plausible value.



### Remove unlogical entries and 5% outliers

Here the second mentioned variant for the data, where we remove the unlogical students with high income and non students with the income of 0$.

```{r, echo=FALSE}
#remove non students with 0 income
data_remove <- data_wage[!(data_wage$wage == 0 & data_wage$job_role != "Student" | data_wage$wage == 0 & data_wage$industry != "I am a student"), ]

#remove students with income > 150k
data_remove <- data_remove[!(data_remove$wage > 150000 & (data_remove$job_role == "Student" | data_remove$industry == "I am a student")), ]

#create new outlierfunction 5/95%
outlier1 <- function(x){
  quantiles <- quantile(x, c(.05, .95))
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  x
}   
#use outlierfunction
data_remove['wage'] <- map_df(data_remove['wage'], outlier1)

data_remove_n_outlier5 <- data_remove

```


Let's look at the distribution of the wage now:

```{r, echo=FALSE}
# Let's check our dependent variable "wage"
ggplot(data_remove_n_outlier5, aes(x = wage)) + 
  geom_histogram(fill="aquamarine3", color = "black") +
  labs(title = "Histogram of the wages for option 2") +
  theme(legend.position="none")
```

We see a difference in the top and bottom of the wage. But let us see in the Algo witch of the dataset is better.


### Data selection

To the most important part now:
For our model we want to use only the relevants features.
So help us select the right features, we use the boruta algorithm. 



#### Boruta
Here we see the first Boruta results. 

```{r, echo=FALSE,message=FALSE,warning=FALSE}
test_data <- data_outliers10
boruta_output <- Boruta(wage~., data = test_data, doTrace=2)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
```
```{r, echo=FALSE,message=FALSE,warning=FALSE}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```
Since it was not finished after one run, we now create a subset only with the remaining 63 features.
The 63 features are:
 [1] "gender"
 [2] "age"
 [3] "country"
 [4] "education"
 [5] "undergraduate_major"
 [6] "job_role"
 [7] "industry"
 [8] "years_experience"
 [9] "ML_atwork"
[10] "Activities_Analyze.and.understand.data.to.influence.product.or.business.decisions"
[11] "Activities_Build.and.or.run.a.machine.learning.service.that.operationally.improves.my.product.or.workflows"
[12] "Activities_Build.and.or.run.the.data.infrastructure.that.my.business.uses.for.storing..analyzing..and.operationalizing.data"
[13] "Activities_Build.prototypes.to.explore.applying.machine.learning.to.new.areas"
[14] "Activities_Do.research.that.advances.the.state.of.the.art.of.machine.learning"
[15] "Activities_None.of.these.activities.are.an.important.part.of.my.role.at.work"
[16] "Notebooks_Kaggle.Kernels"
[17] "Notebooks_Google.Colab"
[18] "Notebooks_Google.Cloud.Datalab"
[19] "cloud_Amazon.Web.Services..AWS."
[20] "cloud_Microsoft.Azure"
[21] "cloud_Alibaba.Cloud"
[22] "cloud_I.have.not.used.any.cloud.providers"
[23] "Programming_Python"
[24] "Programming_R"
[25] "Programming_SQL"
[26] "Programming_Bash"
[27] "Programming_Java"
[28] "Programming_Visual.Basic.VBA"
[29] "Programming_C.C.."
[30] "Programming_MATLAB"
[31] "Programming_Scala"
[32] "Programming_SAS.STATA"
[33] "Programming_language_used_most_often"
[34] "ML_framework_Scikit.Learn"
[35] "ML_framework_TensorFlow"
[36] "ML_framework_Keras"
[37] "ML_framework_Spark.MLlib"
[38] "ML_framework_H20"
[39] "ML_framework_Caret"
[40] "ML_framework_Xgboost"
[41] "ML_framework_randomForest"
[42] "ML_framework_None"
[43] "Visualization_ggplot2"
[44] "Visualization_Matplotlib"
[45] "Visualization_Shiny"
[46] "Visualization_Plotly"
[47] "percent_actively.coding"
[48] "How.long.have.you.been.writing.code.to.analyze.data."
[49] "For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.."
[50] "Do.you.consider.yourself.to.be.a.data.scientist."
[51] "data_Categorical.Data"
[52] "data_Geospatial.Data"
[53] "data_Image.Data"
[54] "data_Numerical.Data"
[55] "data_Sensor.Data"
[56] "data_Tabular.Data"
[57] "data_text.Data"
[58] "data_Time.Series.Data"
[59] "explainability.model_Examine.individual.model.coefficients"
[60] "explainability.model_examine.feature.correlations"
[61] "explainability.model_Examine.feature.importances"
[62] "explainability.model_LIME.functions"
[63] "explainability.model_SHAP.functions"     


```{r, echo=FALSE,message=FALSE,warning=FALSE}
#create new dataframe
data_outliers10 <- data_outliers10[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,19,23,24,26,27,28,29,30,31,32,34,35,36,37,39,40,41,42,43,45,46,47,48,49,50,51,52,54,55,57,58,59,60,61,63,64,65,66,67,68,69,71,72,73,75,76,78)]

data_remove_n_outlier5 <- data_remove_n_outlier5[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,19,23,24,26,27,28,29,30,31,32,34,35,36,37,39,40,41,42,43,45,46,47,48,49,50,51,52,54,55,57,58,59,60,61,63,64,65,66,67,68,69,71,72,73,75,76,78)]

# run Boruta again with the remaining features
boruta_output <- Boruta(wage~., data = data_outliers10, doTrace=2)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)

```
```{r}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```

with this output we see, that these variables have an importance. So we have our final features. We these we can build our model.

## ML part

### Find best ML

We use H20 AutoML to get the best ML-technique. It creates different machine learning algorithms, and compares them automatically.

```{r H20Cluster} 

# Installing & initializing H20. In case you have past installations, you should run 
# lines 37-44. If this is the first time installing, you can only run lines 43-44.
#if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
#if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
#install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))

# Initialize H2O cluster
h2o.init()


# Specify the column containing the dependent feature
dep_var <- "wage"


# Identify the categorical variables in the dataset
cat_cols <- c()
for (col in names(data_outliers10)) {
  if (class(data_outliers10[[col]]) == "factor" | typeof(data_outliers10[[col]]) == "character") {
    cat_cols <- c(cat_cols, col)
  }
}

# Convert the categorical variables to factors
data_outliers10[cat_cols] <- lapply(data_outliers10[cat_cols], factor)


df <- as.h2o(data_outliers10)


# Split dataset into training and test sets
set.seed(8)
splits <- h2o.splitFrame(df, ratios = c(0.8))
train <- splits[[1]]
test <- h2o.importFile("our_data.csv")
#h2o.importFile("our_data.csv")
#our_data <- read.csv("our_data.csv")

```

# Run AutoML for outliers10

```{r name=automl}
automl <- h2o.automl(
  x = setdiff(colnames(df), dep_var), # independent variables
  y = dep_var, # dependent variable
  training_frame = train,
  max_runtime_secs = 600, # maximum time in seconds for AutoML to run
  seed = 12 # set seed for reproducibility
)


  # View leaderboard of models generated by AutoML
lb <- automl@leaderboard
print(lb, n = nrow(lb)) 
```

We see that the best model which is not a Stacked Ensemble is a GBM model. The Gradient Boosting Machine (GBM) is a machine learning model that combines multiple weak prediction models, typically decision trees, to create a strong predictive model. It does this by iteratively fitting new models to the residuals of the previous models, effectively focusing on the mistakes made by the previous models and minimizing them in subsequent iterations. The final prediction is obtained by aggregating the predictions of all the models, with each model assigned a weight based on its performance.

Now let us see the results for the other dataset:
```{r}


# Identify the categorical variables in the dataset
cat_cols <- c()
for (col in names(data_remove_n_outlier5)) {
  if (class(data_remove_n_outlier5[[col]]) == "factor" | typeof(data_remove_n_outlier5[[col]]) == "character") {
    cat_cols <- c(cat_cols, col)
  }
}

# Convert the categorical variables to factors
data_remove_n_outlier5[cat_cols] <- lapply(data_remove_n_outlier5[cat_cols], factor)


df <- as.h2o(data_remove_n_outlier5)


# Split dataset into training and test sets
set.seed(8)
splits <- h2o.splitFrame(df, ratios = c(0.8))
train <- splits[[1]]
test <- h2o.importFile("our_data.csv")


automl1 <- h2o.automl(
  x = setdiff(colnames(df), dep_var), # independent variables
  y = dep_var, # dependent variable
  training_frame = train,
  max_runtime_secs = 600, # maximum time in seconds for AutoML to run
  seed = 12 # set seed for reproducibility
)


  # View leaderboard of models generated by AutoML
lb <- automl1@leaderboard
print(lb, n = nrow(lb)) 
```


We see that the RMSE here is 27'284$ and so worse to the outliers 10% with 21'106$, so we use the better model with lower RMSE.

Let's have a look at the best of those GMB models.

```{r}
# Find the best performing model per RMSE criteria and explore it.
best_RMSE <- h2o.get_best_model(automl, criterion = "RMSE", algorithm = "GBM")   # Best model per the RSME indicator. 
best_RMSE                                                    # Let's explore the best perfroming model 
```

It has 80 trees, interesting is the RMSE, which is still 21'095 dollar which means, that every output has a deviation of +/- 21'095$ which is not that great for predicting wage. 
Now we predict our salaries(Abishan, Josua, Lars & Luca):

```{r, echo=FALSE}
# Predictions and performance on our test sample. 
# Obtain the predictions for our test subset
pred_best_RMSE <- h2o.predict(best_RMSE, test)
predictions <- as.data.table(pred_best_RMSE)
predictions
```


## Future Salaries
Now we look deeper on every salary.

### Abishan
```{r}
Abishan <- h2o.explain_row(best_RMSE, test, row_index = 1)
Abishan$shap_explain_row$plots

```

His wage is influenced by various factors. For a high wage, being in Switzerland, having an "Other" job_role and having a business discipline undergraduate major are important. For a low wage, being a student, uncertain ML involvement, 0-1 years of experience, and an age of 22-24 are significant.
These factors are not surprising and are therefore plausible.But the value around 23,3k is very low and with the deviation from 2'266 to 44'456 dollar still very low for switzerland. We don't think that makes sense.



### Josua

```{r}
Josua <- h2o.explain_row(best_RMSE, test, row_index = 2)
Josua$shap_explain_row$plots
```

According to our model, important parameters for a high wage include being in Switzerland, having th "Other" job role, using SQL programming language, and having a business discipline undergraduate major. For a low wage, being a student, not working with machine learning, having 0-1 years of experience, and writing code for 1-2 years are significant factors. 
These factors are not surprising and are therefore plausible. With the almost 38k still low for Switzerland. Deviations varying from 16'894 to 59'084 doesn't make it better.

### Lars

```{r}
Lars <- h2o.explain_row(best_RMSE, test, row_index = 3)
Lars$shap_explain_row$plots
```

For a higher wage, being in Switzerland, having the "Other" job role, and having a business discipline undergraduate major are important. For a lower wage, being a student, not working with machine learning, having 0-1 years of experience, and falling within the age range of 25-29 are significant factors. These findings are based on the ML model, and other factors can also impact wages. also here the expected salary is very low for Switzerland. The Range with deviations is: 11'418 to 53'608 dollar

### Luca

```{r}
Luca <- h2o.explain_row(best_RMSE, test, row_index = 4)
Luca$shap_explain_row$plots
```

Wage is influenced by various factors. For a higher wage, being in Switzerland, working in the "Other" job role in the Shipping and Transportation industry, and having well-established involvement in machine learning at work are important. For a lower wage, being in the age range of 25-29, having 3-4 years of experience, and not being engaged in activities for building prototypes are significant. These findings are based on the ML model, and other factors can also impact wages. Here the Range and salary expecations are from our knowledge on point. Range starts from 70'305 to 112'495 dollar.

### Conclusion

Across all four trials, there are clear indications of factors that are relevant to wages. Country is a very strong determinant, which is interesting for us here in Switzerland since there were only 87 entries for our country. Additionally, being a student is an important factor. We can also observe the biggest difference in our statements since Luca was the only one who did mention being in the industry, instead stating "I am a student." As a result, Luca earns three times as much as the rest of us. Age is also a relevant factor; based on our information (everyone under 30 years of age), it has a negative impact on wages. The job role assigned as "Other" is the only strong factor that is not clearly understandable. Nevertheless, we are fundamentally very satisfied with our model, as the factors it considers are plausible and comprehensible. We will be happy once we can settle as students and immediately start earning more than before ;) 

```{r,echo=FALSE}
print(paste("Number of observations from Switzerland: ", nrow(data[data_wage$country == "Switzerland", ])))
```
## SessionInfo
```{r,echo=FALSE}
sessionInfo()
```