---
title: "Groupwork_2_Group6"
author: "Luca Wirths,Lars Wenger"
date: "2023-05-21"
output:
  html_document:
    toc: yes
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of content {.tabset}

## Preparations

### Import Modules
```{r, include=FALSE}
#install.packages("readr")
#install.packages("dplyr")
#install.packages("dlookr")
#install.packages("naniar")
#install.packages("UpSetR")
#install.packages("ggplot2")
#install.packages("corrplot")
#install.packages("tidyr")
#install.packages("mice")
#install.packages("purrr")
#install.packages("scales")
#install pachages("ggcorrplot")

library(readr)
library(dplyr)
library(dlookr)
library(naniar)
library(UpSetR)
library(ggplot2)
library(corrplot)
library(tidyr)
library(mice)
library(purrr)
library(scales)
library(ggcorrplot)
library(ROSE)
library(Boruta)
library(DescTools)
library(caret)
library(ROCR)
library(pROC)
library(plotly)
library(randomForest)
library(rpart)
library(rpart.plot)


```

```{r, eval=FALSE}
rm(list=ls())
libraries = c("readr", "dplyr", "ggplot2","Boruta", "dlookr", "ROCR", "caret", "pROC", "dplyr", "ROSE", "corrplot", "DescTools", "ggpubr", "tidyverse", "RColorBrewer", "ggcorrplot", "PerformanceAnalytics", "corrr", "networkD3", "reshape", "knitr","naniar","UpSetR","tidyr","mice","purrr","scales","plotly","randomForest","rpart","rpart.plot")
 
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})
set.seed(8)
```

### Import Data
```{r}
load("data_wage.RData")
data_wage <- data #copy of dataset
set.seed(8)
```

## Analyse Data LARS

```{r}
overview(data)
summary(data)
```

In the data are 10809 observations of 78 variables. 

Most of the respondents in the dataset are male (9135). Rest of it are female (1571), prefered "not to say" (72) or did "self-describe" (31). Thus the men are strongly overrepresented. However, this is probably due to the industry, where a larger proportion of men are employed. A similar overrepresentation can be seen in the country of origin. There is also a clear majority there with USA (2505) and India (1576).
The dataset is somewhat more balanced with regard to the age of the participants. Most participants are between 25-29 years old (3008), but the remaining age groups are also well represented.
It is also clear that about half of the participants have a master's degree (5209). However, there is also a good proportion of participants with a bachelor's degree (2990) or a doctorate (1869).
Most participants have studied "Computer Science" (4239), "Engineering" (1704) or "Mathematics or Statistics" (1545). 
In terms of job title, there are different designations with "Data Scientist" (2505), "Software Engineer" (1800), "Student" (1588) and "Data Analyst" (1022).
The majority of the participants still have little professional experience. Thus, more than half of the participants have less than 3 years of experience.


### Handle outliers

We now investigated if there are outliers. 
To have a better feeling and see more in detail how the feature is distributed we show here the plot_outlier, which gives us a boxplot and a histogram with and without outliers.

```{r,echo=FALSE}
#Let's visualize all columnns with and without outliers
data %>%
  plot_outlier(col="aquamarine3",diagnose_outlier(data['wage']) %>%
                 filter(outliers_ratio >= 0.5) %>%          # dplyr
                 select(variables) %>%
                 unlist())

```

We see that without the outliers the standarddistribution is better. So we create now the standard function to replace the outliers with the 5th and 95th percentile value of that wage feature.

```{r,echo=FALSE}
outlier <- function(x){
  quantiles <- quantile(x, c(.05, .95))
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  x
}   
```

In the next step, we apply the outlier function. For this, we use the map_df function. This allows us to apply a function to each element of a list or atomic vector.

```{r}

#Use function outlier for the dataset
data['wage'] <- map_df(data['wage'], outlier)
```

### Target feature wage

let us check the target feature. It does not really make sense, if you think why are there people with that low of an income. But with all the different costs of living it makes sense.

```{r}
# Let's check our dependent variable "wage"
summary(data$wage)
ggplot(data, aes(x = wage)) + 
  geom_histogram(fill="aquamarine3", color = "black") +
  labs(title = "Histograme of the wages") +
  theme(legend.position="none")
```


### Data selection

To the most important part now:
For our model we want to use only the relevants features.
So help us select the right features, we use the boruta algorithm. 



#### TEST features
Here we see the first Boruta results. 

```{r, echo=FALSE,message=FALSE,warning=FALSE}
test_data <- data
str(test_data)
boruta_output <- Boruta(wage~., data = test_data, doTrace=2)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)
```
```{r, echo=FALSE,message=FALSE,warning=FALSE}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```
Since it was not finished after one run, we now create a subset only with the remaining 53 features.
The 53 features are: (Evtl am ende lÃ¶schen da die Anzeige oben ist.. )
 [1] "age"
 [2] "country"
 [3] "education"
 [4] "undergraduate_major"
 [5] "job_role"
 [6] "industry"
 [7] "years_experience"
 [8] "ML_atwork"
 [9] "Activities_Analyze.and.understand.data.to.influence.product.or.business.decisions"
[10] "Activities_Build.and.or.run.a.machine.learning.service.that.operationally.improves.my.product.or.workflows"
[11] "Activities_Build.and.or.run.the.data.infrastructure.that.my.business.uses.for.storing..analyzing..and.operationalizing.data"
[12] "Activities_Build.prototypes.to.explore.applying.machine.learning.to.new.areas"
[13] "Activities_Do.research.that.advances.the.state.of.the.art.of.machine.learning"
[14] "Activities_None.of.these.activities.are.an.important.part.of.my.role.at.work"
[15] "Notebooks_Google.Colab"
[16] "Notebooks_Google.Cloud.Datalab"
[17] "cloud_Amazon.Web.Services..AWS."
[18] "cloud_Alibaba.Cloud"
[19] "cloud_I.have.not.used.any.cloud.providers"
[20] "Programming_Python"
[21] "Programming_R"
[22] "Programming_SQL"
[23] "Programming_Java"
[24] "Programming_C.C.."
[25] "Programming_MATLAB"
[26] "Programming_language_used_most_often"
[27] "ML_framework_Scikit.Learn"
[28] "ML_framework_TensorFlow"
[29] "ML_framework_Keras"
[30] "ML_framework_Spark.MLlib"
[31] "ML_framework_Caret"
[32] "ML_framework_Xgboost"
[33] "ML_framework_randomForest"
[34] "ML_framework_None"
[35] "Visualization_ggplot2"
[36] "Visualization_Matplotlib"
[37] "Visualization_Shiny"
[38] "Visualization_Plotly"
[39] "Visualization_None"
[40] "percent_actively.coding"
[41] "How.long.have.you.been.writing.code.to.analyze.data."
[42] "For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.."
[43] "Do.you.consider.yourself.to.be.a.data.scientist."
[44] "data_Categorical.Data"
[45] "data_Image.Data"
[46] "data_Numerical.Data"
[47] "data_Tabular.Data"
[48] "data_text.Data"
[49] "data_Time.Series.Data"
[50] "explainability.model_Examine.individual.model.coefficients"
[51] "explainability.model_examine.feature.correlations"
[52] "explainability.model_Examine.feature.importances"
[53] "explainability.model_None.I.do.not.use.these.model.explanation.techniques"  


```{r, echo=FALSE,message=FALSE,warning=FALSE}
#create new dataframe
new_data <- data[,c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,19,23,26,27,28,29,30,32,35,36,40,41,42,43,45,47,48,49,50,51,52,54,55,56,57,58,59,60,61,64,65,67,68,69,71,72,73,77,78)]

# run Boruta again now with all data to the selected features
boruta_output <- Boruta(wage~., data = new_data, doTrace=2)
boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)

```
```{r}
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```

with this output we see, that the variable Google Colab is not that important so we remove it. 

```{r}
new_data <- new_data %>% select(-Notebooks_Google.Colab)
```

With that we we have no our final features. We these we can build our model.

## ML part LUCA

### Find best ML

We use H20 AutoML to get the best ML-technique. It creates different machine learning algorithms, and compares them automatically.

 Installing & initializing H20. 
```{r H20Cluster} 

# Installing & initializing H20. In case you have past installations, you should run 
# lines 37-44. If this is the first time installing, you can only run lines 43-44.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)


# Call other necessary libraries 
library(data.table)
library(writexl)
library(readr)


# Initialize H2O cluster
h2o.init()


# Specify the column containing the dependent feature
dep_var <- "wage"


# Identify the categorical variables in the dataset
cat_cols <- c()
for (col in names(new_data)) {
  if (class(new_data[[col]]) == "factor" | typeof(new_data[[col]]) == "character") {
    cat_cols <- c(cat_cols, col)
  }
}

# Convert the categorical variables to factors
new_data[cat_cols] <- lapply(new_data[cat_cols], factor)


df <- as.h2o(new_data)


# Split dataset into training and test sets
set.seed(8)
splits <- h2o.splitFrame(df, ratios = c(0.8))
train <- splits[[1]]
test <- h2o.importFile("our_data.csv")
#h2o.importFile("our_data.csv")
#our_data <- read.csv("our_data.csv")

```

# Run AutoML

```{r name=automl}
automl <- h2o.automl(
  x = setdiff(colnames(df), dep_var), # independent variables
  y = dep_var, # dependent variable
  training_frame = train,
  max_runtime_secs = 600, # maximum time in seconds for AutoML to run
  seed = 12 # set seed for reproducibility
)


  # View leaderboard of models generated by AutoML
lb <- automl@leaderboard
print(lb, n = nrow(lb)) 
```
```{r}

# We can also export the table and observe the result more in-depth.
lb_table <- as.data.table(lb)
#write_xlsx(lb_table, "trained_ML_models.xlsx")        # The excel file will be located in your set working directory. 


# Find the best performing model per a certain criteria and explore it.
best_RMSE <- h2o.get_best_model(automl, criterion = "RMSE")   # Best model per the RSME indicator. 

leaderboard <- h2o.get_leaderboard(automl, extra_columns = "ALL")
second_best_model <- h2o.getModel("StackedEnsemble_AllModels_3_AutoML_2_20230511_141540"))


best_RMSE                                                    # Let's explore the best perfroming model 
```

```{r}

# Get training timing info
trainingInfo <- automl@training_info
trainingInfo

```
```{r}
# Predictions and performance on our test sample. 
# Obtain the predictions for our test subset
pred_best_RMSE <- h2o.predict(best_RMSE, test)
pred <- h2o.predict(second_best_model, test)
```
```{r}
predictions <- as.data.table(pred_best_RMSE)
predictions

predictions2 <- as.data.table(pred)
predictions2
```
```{r}

# Plot the ROC and the Precision Recall curve
plot(pred_best_RMSE, type = "roc")
plot(pred_best_RMSE, type = "pr")
```


## Explanation
h20.explain

# ----------------------------------------------------------------------------- #
# ----------------------------------------------------------------------------- #
# ----------------------- Explainability in H20 ----------- ------------------- #
# ----------------------------------------------------------------------------- #
# ----------------------------------------------------------------------------- #

# H20 framework explained (for further details check the documentation:
# https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html)
# H2O Explainability Interface is a convenient wrapper to a number of explainabilty 
# methods and visualizations in H2O. The main functions:
#  * h2o.explain() (global explanation)
#  * h2o.explain_row() (local explanation) 
# work for individual H2O models, as well a list of models or an H2O AutoML object. 

# The h2o.explain() function generates a list of explanations â individual units 
# of explanation such as a Partial Dependence plot or a Variable Importance plot. 
# Most of the explanations are visual â these plots can also be created by individual 
# utility functions outside the h2o.explain() function. The visualization engine 
# used in the R interface is the ggplot2 package


# When h2o.explain() is provided a list of models, the following global explanations 
# will be generated by default:
#  * Leaderboard (compare all models)
#  * Confusion Matrix for Leader Model (classification only)
#  * Residual Analysis for Leader Model (regression only)
#  * Variable Importance of Top Base (non-Stacked) Model
#  * Variable Importance Heatmap (compare all non-Stacked models)
#  * Model Correlation Heatmap (compare all models)
#  * SHAP Summary of Top Tree-based Model (TreeSHAP)
# * Partial Dependence (PD) Multi Plots (compare all models)
# * Individual Conditional Expectation (ICE) Plots


# Global explanations -- we explain the innerworkings of the model as a whole.

# Explain leader model & compare with all AutoML models
exp_automl <- h2o.explain(best_RMSE, test) 
print(exp_automl) 


# Some explainations concerning the plots: 
# The green lines or dots on the PDP show the average response versus feature value with error bars.  
# The grey histogram shows the number of instances for each range of feature values. 
# For the monthly income plot plot, we can see that the average leaving rate decreases
# with the montly income, but also that the data becomes increasingly sparse for larger interest rates.


# Local explanations -- Explain a specific unit (i.e. obtain explanations for 
# a single person in the sample). 

# In the following step, we explain the behavior of a model or group of models 
# with respect to a single row of data. By using the h2o.explain_row() function, 
# the outputs would be: 
#  * SHAP Contribution Plot (for the top tree-based model in AutoML)
#  * Individual Conditional Expectation (ICE) Plots

# In the next line, we explain how the model performs for a single row (row 4 from
# the validation set)
row_4_GBM_auc <- h2o.explain_row(best_RMSE, test, row_index = 4)
row_4_GBM_auc


## Future Salaries

### Abishan
h20.explain_row()
These are Abishan's inputs:
XXXXXXXX


Abi <- h2o.explain_row(GBM_auc, our_data, row_index = 1)

### Josua

```{r}
Josua <- h2o.explain_row(GBM_auc, our_data, row_index = 2)
```


### Lars
```{r}
Lars <- h2o.explain_row(GBM_auc, our_data, row_index = 3)
```


### Luca
```{r}
Luca <- h2o.explain_row(GBM_auc, our_data, row_index = 4)
```









###### Old work, some things can be used. 



### Import Data
```{r}
data_loans <- read_csv("loan_sample_8.csv")
data <- data_loans # We always make a copy from the original dataset and work on the copy
data <- data[,c(1,2,5,8,9,10,11,12,13,15,16,17,3,4,6,7,14)] # order numerical then categorical
```


## Exercise 1
### Structure and dimensions of the data set
#### What are the dimensions of the data set?

```{r}
dim(data)

```

The result of the function dim() of our data set show that we have a sample of 40'000 rows and 17 columns.

#### How many numeric and how many categorical variables are included in the data?

```{r}
overview(data) 

```

With the function overview() we see that there are 12 numerical and 5 categorical features. 

#### Summarize the variables. Discuss the summary statistics obtained.
```{r}
summary(data)
```

We don't see any specially high or low numbers in our data set.

The loan amount of 40'000 is high but not an error, because the average loan amount is 11'687 and the mean 10'075 which indicates a low number of outlines.

The maximum in the feature int_rate of the interest with 27.49 is in our eyes possible, but we would not take that loan ourselves. Since the mean and median are close, it is acceptable.

An annual income of 400'000 is high, but no error, since a Federal Council in Switzerland makes CHF 456'854(01.01.22). Also here, the mean and median ar quite close. The annual income of 5'000 may be an error, but can be someone only working for 1 month or parttime.

The maximum dti of 60.14 is high, but on a closer look, we see that the majority did not default, so it makes it a valuable data set.

The open_acc is fine, with a max of 23 months left on the loan.

We assume that in revolving credit balance, a customer can take multiple loans. Because only in this way it makes sense that there are, for example, values where the customer takes a loan of 1400 and has a value of 78000 revolving credit balance.

The median and mean of revolving line utilization line is special. The Median is bigger than the mean, which means, that it is a left skewed distribution.

For the total_acc, in our opinion, this data set makes sense. The range between 3 and 57 is possible.

If the loan_amnt is bigger, then the customer also have to pay more interest. So in the column total_rec_int you see, how much they have paid until today. The min of 0 is possible, if the customer has not paid anything until today.

The tot_cur_bal of 472573 max is possible. But the mean and the median are not so close together. And the current total balance of 0 is special, but it could be possible, if for example it is at the end of the month and the payroll is tomorrow.

The total_rev_hi_lim min and max is possible. The Mean and Median are close together.



#### Check the levels of the target variable by choosing the appropriate visualization. Is the target variable balanced?

In the next step, we investigate our target variable. We notice that in our sample, we have 34,810 persons which did not default on their loan and we have 5,190 which did default. 

```{r}
PercTable(data$Status)
```

We can also visualize the count by plotting a bar plot.
```{r, echo=FALSE}
ggplot(data, aes(x = Status, fill = Status)) +
  geom_bar(fill = "aquamarine3",
                 color = "grey") +
  ylab("Count") +
  xlab("Status")
```

The plot indicates that the data set is highly imbalanced. 
The function we use to perform an under sampling is ovun.sample().
```{r, echo=FALSE}
set.seed(8)             # We set the seed so we have reproducable results
data_balanced <- ovun.sample(Status ~ ., data=data, method = "under")
data_under <- data.frame(data_balanced[["data"]])
```

### Investigate whether certain variables contain outliers (hint: what does a box plot show?). Elaborate your view on how to proceed in dealing with the outliers and â if necessary âtake appropriate action.

We now investigated if there are outliers by creating a boxplot for every feature. 

```{r, fig.width=20, fig.height=20, echo=FALSE}
# Check each column if it contains outliers
diagnose_numeric(data_under,1:11)

#Make a dataset where only numeric columns are included
data_new <- data_under[complete.cases(data_under),]
data_num <- data_new %>%
  select_if(is.numeric)

# We could see, that from the numeric columns 11 got outliers and 1 doesn't contain any outlier. Now lets make a boxplot for the graphical visualization of these outliers.
boxplot(scale(data_num[,1:11]), use.cols = TRUE,col="aquamarine3")
```

To have a better feeling and see more in detail how the feature is distributed we show here the plot_outlier, which gives us a boxplot and a histogram with and without outliers.

```{r,echo=FALSE}
#Let's visualize all columnns with and without outliers
data_num %>%
  plot_outlier(col="aquamarine3",diagnose_outlier(data_num) %>%
                 filter(outliers_ratio >= 0.5) %>%          # dplyr
                 select(variables) %>%
                 unlist())

```

We see that without the outliers the standarddistribution is almost everywhere better. So we create now the standard function to replace the outliers with the 5th and 95th percentile value of that feature.
```{r,echo=FALSE}
outlier <- function(x){
  quantiles <- quantile(x, c(.05, .95))
  x[x < quantiles[1]] <- quantiles[1]
  x[x > quantiles[2]] <- quantiles[2]
  x
}   
```

In the next step, we apply the outlier function to our numeric variables in the under-sampled dataset. For this, we use the map_df function. This allows us to apply a function to each element of a list or atomic vector.
```{r}

#Use function outlier for the dataset
data_new_under <- map_df(data_under[,-c(12:17)], outlier)
cols <- data_under[,c(12:17)]
data_new_under <- cbind(data_new_under, cols)

```

After the changes the boxplots look like this:
```{r, fig.width=20, fig.height=20, echo=FALSE}

#Let's see what changed in the visualization of the boxplot after the capping of the outliers
boxplot(scale(data_new_under[,c(1:11)]), use.cols = TRUE,col="aquamarine3")

```



### Choose the appropriate visualization to investigate the distribution of the numericfeatures per the two levels of our target feature (i.e. default vs non-default).

Attention, the Code below give in R Studio the right visualization, but when knittet, not. We tested multiple hours, but did not find a solution.

```{r, echo=FALSE}
for (i in 1:length(data[,-c(12:17)])) {
  print(ggplot(data_new_under, aes(y = data_new_under[,i], x = Status)) + 
          geom_boxplot(fill = "aquamarine3",
                 color = "black") + 
          ylab(names(data_new_under[i]))) 
}
```



### Use a bar plot visualization to investigate the associations between the categoricalvariables and the target feature.

#### Association between Status and grade

```{r, echo=FALSE}

ggplot(data_new_under, aes(x = Status, fill = grade)) +
  geom_bar(position="stack") +
  ylab("Count") +
  xlab("Status")
```

#### Association between Status and home_ownership
```{r, echo=FALSE}
ggplot(data_new_under, aes(x = Status, fill = home_ownership)) +
  geom_bar(position="stack") +
  ylab("Count") +
  xlab("Status")
```

#### Association between Status and verification_status
```{r, echo=FALSE}
ggplot(data_new_under, aes(x = Status, fill = verification_status)) +
  geom_bar(position="stack") +
  ylab("Count") +
  xlab("Status")
```

#### Association between Status and purpose
```{r, echo=FALSE}
ggplot(data_new_under, aes(x = Status, fill = purpose)) +
  geom_bar(position="stack") +
  ylab("Count") +
  xlab("Status")
```

#### Association between Status and application_type
```{r, echo=FALSE}
ggplot(data_new_under, aes(x = Status, fill = application_type)) +
  geom_bar(position="stack") +
  ylab("Count") +
  xlab("Status")

```


### Visualize the correlations that emerge between the numerical features.

#### Boruta Algo

Before we look at the correlations we let the Boruta algorithm give us the importance of every feature. 

```{r, echo=FALSE,message=FALSE,warning=FALSE}
data_new_under$Status <- as.factor(data_new_under$Status)
boruta_output <- Boruta(Status~., data = data_new_under, doTrace=2)
#boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
#print(boruta_signif)
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance") 
```

#### Correlation

Since we have now the importance, we check for multicollinearity which is also a part of the feature selection process.

```{r, echo=FALSE}
correlations = cor(data_new_under[-c(12:17)])
corrplot(correlations) 
```

#### Insignificant correlations

Lastly, we also test for the significance of the correlation. With the cor_pmat function it allows us to visualize the correlations and explicitly marking the insignificant correlations. 

```{r, echo=FALSE}
p_value_mat <- cor_pmat(data_new_under[,-c(12:17)])
ggcorrplot(correlations, 
           type = "lower", 
           p.mat = p_value_mat,
           ggtheme = ggplot2::theme_gray,
           colors = c("#6D9EC1", "white", "#E46726")) 
```


Looking at the correlation plots and the output from the Boruta algorithm, we decide to keep all features. 

### Plot an interactive scatter plot of the association between the loan amount requested and the annual income of the borrower.

```{r,echo=FALSE}
visualisation <- ggplot(data, 
               aes(x = loan_amnt, 
                   y = annual_inc)) + 
  geom_point(colour = "black", alpha = 1/2, shape = 21, fill = "red", size = 2.5)

# Let's make it interactive
ggplotly(visualisation)
```

### Create a new balanced data set where the two levels of the target variable will be equally represented; Create a bar plot of the newly created target variable.


Lets visualize our changed data set.

```{r,echo=FALSE}
ggplot(data_new_under, aes(x = Status, fill = Status)) +
  geom_bar() +
  ylab("Count") +
  xlab("Status")
```


## Exercise 2

### Divide the sample into training and testing set using 80% for training the algorithm.
```{r}

set.seed(8)
data_new_under$Status = as.factor(data_new_under$Status)
div <- createDataPartition(y = data_new_under$Status, p = 0.8, list = F)
data.train <- data_new_under[div,] # 80% here
PercTable(data.train$Status)

data.test <- data_new_under[-div,] # rest of the 20% data goes here
PercTable(data.test$Status)

```
### Train the classifier and report the coefficients obtained and interpret the results.
In the next step, we train the logit model. In terms of our inputs i.e. our Xs, we use all variables included in the data_new_under apart from the status, which is our Y.  

```{r, echo=FALSE}

fit1 <- glm(Status ~ ., data=data.train,family=binomial())
summary(fit1)


```

We print out only the significant variables with p-value lower than 0.05. 
```{r, echo=FALSE}
significant.variables <- summary(fit1)$coeff[-1,4] < 0.05
names(significant.variables)[significant.variables == TRUE]

```

We notice that 14 variables are found statistically significant. 
The listed features are the most important to see if someone is creditworthy.
"loan_amnt", "annual_inc", "int_rate" and "dti": It is obvious that these variables are most important for financial companies.

"open_acc","total_acc","total_rec_int" and "total_rev_hi_lim": Have to do with your overall financial situation and what the company will give us.

"verification_status" has to be ether Verified or Source verified to be significant.

Because there are not all grades, we check them separately:

```{r, echo=FALSE}
visualisation2 <- ggplot(data.train, aes(x = Status, fill = grade)) +
  geom_bar(position="stack") +
  ggtitle("Association between Status and grade")+
  ylab("Count") +
  xlab("Status")

# Let's make it interactive
ggplotly(visualisation2)

```

We see that the amount of people with a grade A who defaulted, is compared to the other grades very low. So "gradeB","gradeC" and "gradeD" are important since the default rate is much higher. 

We were surprised, that the verification_status is not on the list, since we thought many people would cheat there.


### Plot the ROC and the Precision/Recall Curve and interpret the results
Next, we want to test the predictive performance of our model. For this purpose, we plot the ROC curve. 
```{r, echo=FALSE}


# ROC Curve
data.test$Status_score_lg <- predict(fit1, type='response', data.test)
Status_pred <- prediction(data.test$Status_score_lg, data.test$Status)
Status_roc <- performance(Status_pred, "tpr", "fpr")
plot(Status_roc, lwd=1, colorize = TRUE, main = "Status_model: ROC Curve for logistic classifier")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)


```

The shown ROC curve indicates, that the True Positive is best between 0.4 and 0.6 because it is the steepest there. 0.5 looks like the best Value to start because the False positive rate goes up there. 
To be sure we take one oft the best values we tried with the following numbers and results:
TPR 0.4 -> Accuracy of 0.6163
TPR 0.5 -> Accuracy of 0.6408
TPR 0.6 -> Accuracy of 0.6206
With that we take 0.5 since it has the highest Accuracy and also logical it makes the most sense, since the bank takes a 50/50 chance on customers.


In the next step, we visualize the Precision/Recall Curve. This curve summarizes the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.
```{r,echo=FALSE}
# Precision/recall curve
fit1_precision <- performance(Status_pred, measure = "prec", x.measure = "rec")
plot(fit1_precision, main="Fit1: Logit - Precision vs Recall")

```

The Curve shows the trade off between the precision of the results and the recall true results. How many it predicts correct. With the tested TPR of 0.5 we see here the Precision result, which is around 0.7. This is a stable value. 



### Produce the confusion matrix and interpret the results.

```{r,echo=FALSE}
## Confusion matrix
# Predict default if probability is greater than 50%
data.test$Status_predicted_lg <- ifelse(data.test$Status_score_lg > 0.5, "1", "0")
data.test$Status_predicted_lg <- as.factor (data.test$Status_predicted_lg)
confusionMatrix(data=data.test$Status_predicted_lg, reference = data.test$Status)
```

When we look at the Confusion Matrix we see, that with an predicted value of 0, the reference of 0 is predicted in 673 times on point. If we predict a value of 1, in 677 times, the reference will also be 1. That means also, that with 361 values, we have an overestimation. On the other hand, in 367 values, there is an underestimation.

The accuracy is about 65% which is not too accurate. This would mean that there is an error in more than 1/3 of the predictions, which is not acceptable in practice.

In 95% of the cases, the mean will be between 62.87% and 67.02%.



### Report the AUC values and the overall accuracy and interpret the results.
```{r,echo=FALSE}
# AUC value
Status_model_auc <- performance(Status_pred, measure = "auc")
cat("AUC: ",Status_model_auc@y.values[[1]]*100)

```

With our AUC of 71%, the model has a chance of 71% to separate from true positive rate and false positive rate. To take that in a practical way, this means, it is an acceptable discrimination, but has to be at least 81% to be an excellent model.



## Exercise 2 with all data

```{r}

set.seed(8)
data$Status = as.factor(data$Status)
div <- createDataPartition(y = data$Status, p = 0.8, list = F)
data.train <- data[div,] # 80% here
PercTable(data.train$Status)

data.test <- data[-div,] # rest of the 20% data goes here
PercTable(data.test$Status)

```

In the next step, we train the logit model. In terms of our inputs i.e. our Xs, we use all variables included in the data_new_under apart from the status, which is our Y. 

```{r, echo=FALSE}

fitall <- glm(Status ~ ., data=data.train,family=binomial())
summary(fitall)
```

We can print out only the significant variables with p-value lower than 0.05. We notice that 15 variables are found statistically significant. The one more is "tot_cur_bal" which indicates the total of all the accounts.

```{r, echo=FALSE}
significant.variables <- summary(fitall)$coeff[-1,4] < 0.05
names(significant.variables)[significant.variables == TRUE]
```

Next, we want to test the predictive performance of our model. For this purpose, we plot the ROC curve. 

```{r, echo=FALSE}


# ROC Curve
data.test$Status_score_lg <- predict(fitall, type='response', data.test)
Status_pred <- prediction(data.test$Status_score_lg, data.test$Status)
Status_roc <- performance(Status_pred, "tpr", "fpr")
plot(Status_roc, lwd=1, colorize = TRUE, main = "Status_model: ROC Curve for logistic classifier")
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)


```

In the next step, we visualize the Precision/Recall Curve. This curve summarizes the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.

```{r,echo=FALSE}
# Precision/recall curve
fitall_precision <- performance(Status_pred, measure = "prec", x.measure = "rec")
plot(fitall_precision, main="FitAll: Logit - Precision vs Recall")

## Confusion matrix
# Predict default if probability is greater than 50%
data.test$Status_predicted_lg <- ifelse(data.test$Status_score_lg > 0.5, "1", "0")
data.test$Status_predicted_lg <- as.factor (data.test$Status_predicted_lg)
confusionMatrix(data=data.test$Status_predicted_lg, reference = data.test$Status)



# AUC value
Status_model_auc <- performance(Status_pred, measure = "auc")
cat("AUC: ",Status_model_auc@y.values[[1]]*100)

```

We see a lower AUC score. We think that this comes only from the fact, that the data set is not even in the feature "Status"

## Exploration

#### RandomForest

We like to test if a RandomForest could get a better result. 
```{r}
set.seed(8)
data_new_under$Status = as.factor(data_new_under$Status)
div <- createDataPartition(y = data_new_under$Status, p = 0.8, list = F)
data.train <- data_new_under[div,] # 80% here
PercTable(data.train$Status)

data.test <- data_new_under[-div,] # rest of the 20% data goes here
PercTable(data.test$Status)
fit2 <- randomForest(Status ~ ., data = data.train, ntree=5 , mtry= 4, importance=TRUE)

# Make predictions on the test data
data.test$pred_RF <- predict(fit2, type='class', data.test)

# Examine the confusion matrix
table(data.test$pred_RF, data.test$Status)

# Compute the accuracy on the test dataset
mean(data.test$pred_RF == data.test$Status)

```

We see a lower mean accuracy in the RandomForest.

#### Decision Tree
Now we like to test if a Decision tree could get a better result. 
```{r}

## Train model explaining default with selected variables as inputs (i.e. Xs)
# Specify: maximum tree depth and minimum split count
fit3 <- rpart(Status~., data = data.train, method = "class", control = rpart.control(cp = 0, maxdepth =4))

# Plot the decision tree
rpart.plot(fit3, type=5)

# Make predictions on the test data
data.test$pred_DT <- predict(fit3, type='class', data.test)

# Examine the confusion matrix
table(data.test$pred_DT, data.test$Status)

# Compute the accuracy on the test dataset
mean(data.test$pred_DT == data.test$Status)


```

Also the Accuracy in the decison tree is lower than the linear model.



## Exercise 3 
Thinking about the pre-processing steps that you carried out before training the logistic classifier:

### Can you think of a way to improve the predictive performance of your data?
Use the Fold method, to train the model on more cases, we tried that, but didn't understand where to use the fold exactly.
Another way to get a better result is to have more data.

### What can you do differently?
Also get the information about the age of a person, their job description with the work industry and its educational background. Furthermore, the relationship- and family-status.

Age of the Person -> young people have the opportunity to get more salary in some years, older people, who are short before retirement, will then have a significant lower income, which may let them default.

Job description with work industry -> If an industry has a high unemployed rate we could anticipate if someone will default or not, also we can see if a job has a bigger future than others.

Educational Background -> to indicate if a person finds a job quicker in case of an possible unemployment.

Relationship status-> makes the expenses of a person lower, so it has more money to use and is more financially stable.

Family-status -> if someone has children and is divorced or single-parent, the income is lower and expenses are relatively higher.


## Exercise 4
### What kind of challenges may a company face if it would use your model in their daily business, in particular in regard to ethical challenges and moral obligations companies have?

If a company choose, to use our model, there would be a fault rate of around 30%. So it depends on the amount of loan, but there is a chance, to loose a lot of money, if it does not pay out.

With our model, the decision, if somebody get a loan or not, would be only based on the numbers and not on the purpose. If for example somebody urgently needs money to pay an important medical surgery in order to survive the cancer, the company only will decide, if they get the money based on their income, ... In a ethical way, you have to give the money to this person, because otherwise the person who needs the money will die. This is an ethical challenge and also an moral obligations a company has with our model.

To get back to our fault rate of 30% there is a chance, that in such a situation, the person who would be in charge to get the loan, would not get the loan and therefor can not pay the surgery.

But there would also be the chance, that a person who use the money for their criminal businesses, would get the loan. Because our model only analyse the raw numbers and does not care about the status of the person in the civilization.

### Can you think of a way how companies can overcome or at least mitigate the issues that you described above?

They just should not use our model primary. They should use it as an consulting model, together with other analyzing tools. For example a background check of the customer or a personal interview with them. This way, the company can better decide, if the person needs the money and they can base their decisions on more than just one tool.

## SessionInfo
```{r,echo=FALSE}
sessionInfo()
```